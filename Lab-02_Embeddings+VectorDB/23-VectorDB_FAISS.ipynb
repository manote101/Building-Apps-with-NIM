{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNaP3OO9eLyv0jdtvIZnRQe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"BG4Kn4_-1SvY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754387841247,"user_tz":-420,"elapsed":24140,"user":{"displayName":"Wiz101","userId":"06454803732057249760"}},"outputId":"d84cf6bb-9dc0-440e-aeaf-e739a6c2297d"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q langchain-community langchain-nvidia-ai-endpoints faiss-cpu"]},{"cell_type":"code","source":["# For Colab user\n","import os\n","from google.colab import userdata\n","os.environ['NVIDIA_API_KEY'] = userdata.get('NVIDIA_API_KEY')"],"metadata":{"id":"cPpz2oXh1p9K","executionInfo":{"status":"ok","timestamp":1754387841700,"user_tz":-420,"elapsed":451,"user":{"displayName":"Wiz101","userId":"06454803732057249760"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n","from langchain.vectorstores import FAISS\n","from langchain.schema import Document\n","import numpy as np\n","\n","# Initialize NVIDIA services\n","# Get API key from: https://build.nvidia.com\n","\n","# --- Configuration ---\n","LLM_ENDPOINT = \"https://integrate.api.nvidia.com/v1\"\n","LLM_MODEL = \"meta/llama-3.2-3b-instruct\"\n","# LLM_MODEL =\"nvidia/llama-3.1-nemotron-nano-vl-8b-v1\"\n","EMBEDDING_ENDPOINT = \"https://integrate.api.nvidia.com/v1\"\n","EMBEDDING_MODEL = \"nvidia/llama-3.2-nv-embedqa-1b-v2\"\n","\n","llm = ChatNVIDIA(model=\"meta/llama-3.2-3b-instruct\")\n","embedder = NVIDIAEmbeddings(model=\"nvidia/llama-3.2-nv-embedqa-1b-v2\")\n","\n","# Sample documents for our knowledge base\n","documents = [\n","    \"NVIDIA RTX 4090 is the fastest gaming GPU available today\",\n","    \"CUDA is NVIDIA's parallel computing platform and programming model\",\n","    \"GeForce RTX series cards support real-time ray tracing\",\n","    \"Tensor cores accelerate AI and machine learning workloads\",\n","    \"DLSS uses AI to upscale games while maintaining performance\",\n","    \"NVIDIA Omniverse enables real-time collaboration for 3D creators\"\n","]\n","\n","# Create document objects\n","docs = [Document(page_content=doc) for doc in documents]\n","\n","# Generate embeddings and create vector store\n","vector_store = FAISS.from_documents(docs, embedder)"],"metadata":{"id":"GI-Sd94A1Utb","executionInfo":{"status":"ok","timestamp":1754387844661,"user_tz":-420,"elapsed":2960,"user":{"displayName":"Wiz101","userId":"06454803732057249760"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Example 1: Basic Similarity Search\n","def basic_similarity_search(query):\n","    print(f\"Query: {query}\")\n","    results = vector_store.similarity_search(query, k=3)\n","    for i, doc in enumerate(results):\n","        print(f\"{i+1}. {doc.page_content}\")\n","    print(\"-\" * 50)\n","\n","# Basic similarity searches\n","basic_similarity_search(\"graphics card performance\")\n","basic_similarity_search(\"AI acceleration technology\")"],"metadata":{"id":"Y7O0MUbeO16h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754387845967,"user_tz":-420,"elapsed":1304,"user":{"displayName":"Wiz101","userId":"06454803732057249760"}},"outputId":"4a13b7e1-c573-4bf5-d110-762b6e901857"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Query: graphics card performance\n","1. NVIDIA RTX 4090 is the fastest gaming GPU available today\n","2. DLSS uses AI to upscale games while maintaining performance\n","3. GeForce RTX series cards support real-time ray tracing\n","--------------------------------------------------\n","Query: AI acceleration technology\n","1. Tensor cores accelerate AI and machine learning workloads\n","2. DLSS uses AI to upscale games while maintaining performance\n","3. CUDA is NVIDIA's parallel computing platform and programming model\n","--------------------------------------------------\n"]}]},{"cell_type":"code","source":["# Example 2: Search with Similarity Scores\n","def search_with_scores(query):\n","    print(f\"Query: {query}\")\n","    results = vector_store.similarity_search_with_score(query, k=3)\n","    for i, (doc, score) in enumerate(results):\n","        print(f\"{i+1}. [Score: {score:.4f}] {doc.page_content}\")\n","    print(\"-\" * 50)\n","\n","# Search with similarity scores\n","search_with_scores(\"real-time rendering\")"],"metadata":{"id":"CjS97dqwO7TO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754387886834,"user_tz":-420,"elapsed":384,"user":{"displayName":"Wiz101","userId":"06454803732057249760"}},"outputId":"bbc785c7-4e82-4501-a53b-463378c9c94f"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Query: real-time rendering\n","1. [Score: 1.4381] GeForce RTX series cards support real-time ray tracing\n","2. [Score: 1.4454] NVIDIA Omniverse enables real-time collaboration for 3D creators\n","3. [Score: 1.6696] DLSS uses AI to upscale games while maintaining performance\n","--------------------------------------------------\n"]}]},{"cell_type":"code","source":["# Example 3: LLM-Augmented Response\n","def rag_search(query):\n","    print(f\"User Query: {query}\")\n","\n","    # Retrieve relevant documents\n","    retrieved_docs = vector_store.similarity_search(query, k=2)\n","    context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n","\n","    # Generate response using LLM\n","    prompt = f\"\"\"\n","    Context: {context}\n","\n","    Question: {query}\n","\n","    Answer based only on the provided context:\n","    \"\"\"\n","\n","    print(f\"PROMPT: {prompt}\")\n","\n","    response = llm.invoke(prompt)\n","    print(f\"==>\\nAI Response: {response.content}\")\n","    print(\"-\" * 50)\n","\n","\n","# Retrieval-Augmented Generation (RAG)\n","rag_search(\"What is DLSS and how does it work?\")\n","rag_search(\"Explain Tensor cores and their applications\")\n"],"metadata":{"id":"rYwDIdysE_Vs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754387852039,"user_tz":-420,"elapsed":5692,"user":{"displayName":"Wiz101","userId":"06454803732057249760"}},"outputId":"402c4155-911a-4062-fad0-e80e1bc06906"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["User Query: What is DLSS and how does it work?\n","PROMPT: \n","    Context: DLSS uses AI to upscale games while maintaining performance\n","GeForce RTX series cards support real-time ray tracing\n","\n","    Question: What is DLSS and how does it work?\n","\n","    Answer based only on the provided context:\n","    \n","==>\n","AI Response: DLSS (Deep Learning Super Sampling) is a technology that uses AI to upscale games while maintaining performance.\n","--------------------------------------------------\n","User Query: Explain Tensor cores and their applications\n","PROMPT: \n","    Context: Tensor cores accelerate AI and machine learning workloads\n","CUDA is NVIDIA's parallel computing platform and programming model\n","\n","    Question: Explain Tensor cores and their applications\n","\n","    Answer based only on the provided context:\n","    \n","==>\n","AI Response: Based on the provided context, here's an explanation of Tensor cores and their applications:\n","\n","Tensor cores are specialized GPU hardware units that accelerate AI and machine learning (ML) workloads. They are designed to efficiently perform matrix multiplications, which are a key operation in ML algorithms.\n","\n","Tensor cores work by performing multiple calculations in parallel, utilizing the massive parallel processing capabilities of modern GPUs. This enables faster execution times and improved performance in ML workloads, which require intense matrix multiplications.\n","\n","Tensor cores are particularly useful in applications that involve:\n","\n","1. Deep learning: Neural networks, a type of ML model, rely heavily on matrix multiplications. Tensor cores accelerate these operations, making deep learning models more efficient and faster to train.\n","2. Computer vision: Image recognition, object detection, and image processing tasks in computer vision rely on ML algorithms, which benefit from tensor core acceleration.\n","3. Natural language processing: ML models used in NLP, such as language translation and sentiment analysis, can also be accelerated by tensor cores.\n","\n","In summary, tensor cores are specialized hardware units that accelerate AI and machine learning workloads by performing parallel matrix multiplications, making them ideal for applications that involve deep learning, computer vision, and natural language processing.\n","--------------------------------------------------\n"]}]}]}