{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8HjrzyIK5T4KccQw1BmUT"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to LangChain"
      ],
      "metadata": {
        "id": "Ndu_i-FzAFbh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hakEUYno5lBf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Colab user\n",
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['NVIDIA_API_KEY'] = userdata.get('NVIDIA_API_KEY')"
      ],
      "metadata": {
        "id": "XdNjAJIs5Lv0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download sample data\n",
        "!git clone https://github.com/manote101/Building-Apps-with-NIM.git"
      ],
      "metadata": {
        "id": "bhAcn9CnUREb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d24ffe1-5499-44b6-8bcb-dc16ccc40a8d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Building-Apps-with-NIM' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r Building-Apps-with-NIM/requirements.txt"
      ],
      "metadata": {
        "id": "leXZltVB7ZPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q --upgrade langchain langchain-community langchain-nvidia_ai_endpoints faiss-cpu requests==2.32.4\n",
        "\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_core.runnables import RunnableMap, RunnablePassthrough\n",
        "\n",
        "# 1. Load and split documents\n",
        "loader = TextLoader(\"Building-Apps-with-NIM/data/doc1.txt\")  # replace with your filecd\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# 2. Create embeddings and vectorstore\n",
        "embeddings = NVIDIAEmbeddings(model=\"nvidia/llama-3.2-nv-embedqa-1b-v2\")\n",
        "vectorstore = FAISS.from_documents(splits, embeddings)\n",
        "\n",
        "# 3. Create retriever\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# 4. Create LLM and prompt\n",
        "llm = ChatNVIDIA(model=\"meta/llama-3.2-3b-instruct\", temperature=0)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are a helpful AI assistant. Use the following context to answer the user's question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\")\n",
        "\n",
        "# 5. Create a retrieval-augmented chain\n",
        "retrieval_chain = RunnableMap({\n",
        "    # \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
        "    \"context\": lambda x: retriever.invoke(x[\"question\"]),  # Use invoke()\n",
        "    \"question\": RunnablePassthrough(),  # Pass the question unchanged\n",
        "}) | prompt | llm\n",
        "\n",
        "# 6. Call invoke() to run it\n",
        "result = retrieval_chain.invoke({\"question\": \"บริษัทใดมีการนำ Nemo microservices มาใข้แล้วบ้าง\"})\n",
        "\n",
        "# 7. Print result\n",
        "print(result.content)\n"
      ],
      "metadata": {
        "id": "_TN2XYIR7gVv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d490d2d-00b0-4c34-bed2-2a8fedaeca1b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ตามข้อมูลที่ให้มา บริษัทต่อไปนี้มีการนำ NeMo microservices มาใช้แล้ว:\n",
            "\n",
            "1. Cloudera\n",
            "2. Datadog\n",
            "3. Dataiku\n",
            "4. DataRobot\n",
            "5. DataStax\n",
            "6. SuperAnnotate AI\n",
            "7. Weights & Biases\n",
            "8. AT&T (ใช้ NeMo Customizer และ Evaluator เพื่อเพิ่มความแม่นยำของ AI Agent โดย fine-tune โมเดล Mistral 7B สำหรับบริการส่วนบุคคล)\n"
          ]
        }
      ]
    }
  ]
}