{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO+8cX0TVupsDUDZOCRyIRk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Introduction to LangChain"],"metadata":{"id":"Ndu_i-FzAFbh"}},{"cell_type":"code","source":["!pip install -q langchain-community langchain-nvidia_ai_endpoints faiss-cpu"],"metadata":{"id":"hakEUYno5lBf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# For Colab user\n","import os\n","from google.colab import userdata\n","os.environ['NVIDIA_API_KEY'] = userdata.get('NVIDIA_API_KEY')"],"metadata":{"id":"XdNjAJIs5Lv0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# download sample data\n","!git clone https://github.com/manote101/Building-Apps-with-NIM.git"],"metadata":{"id":"bhAcn9CnUREb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n","from langchain.prompts import ChatPromptTemplate\n","from langchain.vectorstores import FAISS\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_community.document_loaders import TextLoader\n","from langchain_core.runnables import RunnableMap, RunnablePassthrough\n","\n","# 1. Load and split documents\n","loader = TextLoader(\"Building-Apps-with-NIM/data/doc1.txt\")  # replace with your file\n","docs = loader.load()\n","\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n","splits = text_splitter.split_documents(docs)\n","\n","# 2. Create embeddings and vectorstore\n","embeddings = NVIDIAEmbeddings(model=\"nvidia/llama-3.2-nv-embedqa-1b-v2\")\n","vectorstore = FAISS.from_documents(splits, embeddings)\n","\n","# 3. Create retriever\n","retriever = vectorstore.as_retriever()\n","\n","# 4. Create LLM and prompt\n","llm = ChatNVIDIA(model=\"meta/llama-3.2-3b-instruct\", temperature=0)\n","\n","prompt = ChatPromptTemplate.from_template(\"\"\"\n","You are a helpful AI assistant. Use the following context to answer the user's question.\n","\n","Context:\n","{context}\n","\n","Question:\n","{question}\n","\n","Answer:\n","\"\"\")\n","\n","# 5. Create a retrieval-augmented chain\n","retrieval_chain = RunnableMap({\n","    # \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n","    \"context\": lambda x: retriever.invoke(x[\"question\"]),  # Use invoke()\n","    \"question\": RunnablePassthrough(),  # Pass the question unchanged\n","}) | prompt | llm\n","\n","# 6. Call invoke() to run it\n","result = retrieval_chain.invoke({\"question\": \"บริษัทใดมีการนำ Nomo microservices มาใข้แล้วบ้าง\"})\n","\n","# 7. Print result\n","print(result.content)\n"],"metadata":{"id":"_TN2XYIR7gVv"},"execution_count":null,"outputs":[]}]}